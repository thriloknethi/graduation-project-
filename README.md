Music recommendation systems have historically relied mainly on features like
genre, artist, and listening history to generate playlists. But the emotional aspect
of music, which is a key driver of user preferences, has generally been
overlooked. This work introduces Emotune, an emotion-based music
recommendation system that uses emotion recognition algorithms to generate
personalized music recommendations according to the emotional state of the
user. Through analysis of the user's emotional state from facial expressions, voice
tone, or text, Emotune generates playlists that resonate with their current mood,
thus improving the overall listening experience. The system uses advanced
emotion classification techniques, such as machine learning models and
sentiment analysis, to accurately identify emotions. We evaluate the performance
of Emotune using user studies and comparative analysis, demonstrating how it
can improve user satisfaction. This work brings out the revolutionary potential of
emotion-based recommendations to transform music platforms by creating
tighter emotional connections between users and music.
